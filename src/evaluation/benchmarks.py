"""
Standard LM Benchmarks via lm-evaluation-harness

Runs: HellaSwag, ARC-Easy, ARC-Challenge, Winogrande, MMLU, GSM8K
"""
# TODO: Implement run_benchmarks()
