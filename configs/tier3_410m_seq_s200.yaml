# Tier 3 Sequential: Single-Pass CCT Training — 410M parameters
# Hardware: 1x RTX 4060 Ti 16GB
# Step Length 200 — same as 160M experiments for clean comparison
#
# Hypothesis: 160M model achieves +35% deployment delta with architecture
#             validated (attention routing works, summaries encoded).
#             410M model (2.5x params, 2x layers, wider) should have
#             enough capacity for cross-step retrieval. Target: <+20%.
#
# Changes from tier1_160m_seq_s200.yaml:
#   1. base_model: pythia-410m-deduped (was 160m)
#   2. d_model: 1024 (was 768)
#   3. n_layers: 24 (was 12), n_heads: 16 (was 12)
#   4. batch_size: 2 (was 4), grad_accum: 4 (was 2) — same effective batch
#   5. learning_rate: 2.0e-5 (was 3.0e-5) — lower for larger model
#   6. decoder_bottleneck: 1024 (same — tests capacity vs architecture)

# ═══════════════════════════════════════
# MODEL
# ═══════════════════════════════════════
base_model: "EleutherAI/pythia-410m-deduped"
d_model: 1024
n_layers: 24
n_heads: 16
vocab_size: 50304

# ═══════════════════════════════════════
# CCT ARCHITECTURE
# ═══════════════════════════════════════
d_summary: 128
d_bottleneck: 256
step_length: 200
step_boundary_mode: "fixed"

# Sequential training mode — eliminates 2-pass distribution mismatch
training_mode: "sequential"

# Full detach required for memory-efficient per-step backward
gradient_isolation: "full_detach"

# Commitment head: no Tanh, no L2 norm (same as 160M)
use_tanh: false
use_l2_norm: false

# L_conclusion_from_premises: predict first N tokens of next step from summaries only
conclusion_n_tokens: 10

# Decoder: MLP
decoder_type: "mlp"
decoder_bottleneck: 1024

# ═══════════════════════════════════════
# TRAINING
# ═══════════════════════════════════════
mode: "finetune"
total_steps: 20000
batch_size: 2
seq_len: 512
gradient_accumulation_steps: 4

optimizer: "adamw"
learning_rate: 2.0e-5
weight_decay: 0.01
warmup_steps: 500
lr_scheduler: "cosine"
max_grad_norm: 1.0

mixed_precision: "bf16"

# ═══════════════════════════════════════
# CURRICULUM
# ═══════════════════════════════════════
curriculum:
  phase1_end: 0.0
  phase2_end: 0.20
  phase3_end: 0.50

loss_weights:
  alpha_start: 1.0
  alpha_end: 0.5
  beta_max: 0.5
  gamma_max: 0.3
  delta_max: 0.2
  delta_start: 0.1

# ═══════════════════════════════════════
# DATA
# ═══════════════════════════════════════
dataset: "HuggingFaceFW/fineweb"
dataset_streaming: true
dataset_split: "train"
validation_dataset: "HuggingFaceFW/fineweb"
validation_split: "train"
num_workers: 4

# ═══════════════════════════════════════
# EVALUATION
# ═══════════════════════════════════════
eval_interval: 2000
eval_steps: 100

eval_tasks:
  - sufficiency_probe
  - compression_ratio
  - lm_perplexity

success_criteria:
  sufficiency_probe_r2: 0.85
  perplexity_delta_pct: 0.10

# ═══════════════════════════════════════
# BASELINE
# ═══════════════════════════════════════
train_baseline: true
baseline_steps: 20000

# ═══════════════════════════════════════
# LOGGING
# ═══════════════════════════════════════
wandb_project: "cct-training"
wandb_name: "tier3-seq-s200-410m"
log_interval: 50
save_interval: 5000
save_dir: "./checkpoints-tier3-seq-s200"
results_dir: "./results-tier3-seq-s200"

# ═══════════════════════════════════════
# HARDWARE
# ═══════════════════════════════════════
gpu_count: 1
gpu_type: "RTX-4060Ti-16GB"
seed: 42
deterministic: true
